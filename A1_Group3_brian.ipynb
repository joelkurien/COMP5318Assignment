{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: Assignment1-groups 3\n",
    "##### Student 1 SID: 540784343\n",
    "##### Student 2 SID: 520330054  \n",
    "##### Student 3 SID: ... \n",
    "##### Student 4 SID: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "rice = pd.read_csv('rice-final2.csv')\n",
    "\n",
    "# print(f'dataset shape: {rice.shape}')\n",
    "# print(f'columns name: {rice.columns.tolist()}')\n",
    "# print(rice.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "# separate feature and target\n",
    "feature = rice.columns[:-1]\n",
    "target = rice.columns[-1]\n",
    "# print(feature)\n",
    "# print(target)\n",
    "\n",
    "\n",
    "# 1.missing attribute values -> mean value\n",
    "X_raw = rice[feature].apply(pd.to_numeric, errors='coerce')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_impute = imputer.fit_transform(X_raw)\n",
    "# print(X_impute[:, 0].mean())\n",
    "# print(X_impute[155])\n",
    "# print(X_impute[6])\n",
    "\n",
    "\n",
    "# 2.MinMax normalisation\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_impute)\n",
    "\n",
    "\n",
    "#3. replace class1 -> 0, class2 -> 1\n",
    "y_raw = rice[target].astype(str).str.strip()\n",
    "RiceClass = {'class1': 0, 'class2': 1}\n",
    "y = y_raw.map(RiceClass)\n",
    "# print(y.head(10))\n",
    "\n",
    "\n",
    "#into numpy array\n",
    "X = X_scaled.astype(float)\n",
    "y = y.to_numpy().astype(int)\n",
    "\n",
    "# 4.Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "print_data(X, y, n_rows=10)         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: \")\n",
    "print(\"NB average cross-validation accuracy: \")\n",
    "print(\"DT average cross-validation accuracy: \")\n",
    "print(\"Bagging average cross-validation accuracy: \")\n",
    "print(\"AdaBoost average cross-validation accuracy: \")\n",
    "print(f\"GB average cross-validation accuracy: {gbClassifier(X, y, gb_n_estimators, gb_n_estimators):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    \n",
    "    return #(appropriate values so that the required printing can be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    param_grid = {'n_estimators':[10, 30, 60, 100], 'max_leaf_nodes': [6, 12]}\n",
    "    clf = GridSearchCV(RandomForestClassifier(max_features='sqrt'), param_grid=param_grid, cv=cvKFold, return_train_score=True)\n",
    "    clf.fit(X, y)\n",
    "    y_predicted = clf.predict(X)\n",
    "\n",
    "    report = classification_report(y, y_predicted, output_dict=True)\n",
    "\n",
    "    results = {\n",
    "        \"best_n_estimators\": clf.best_params_[\"n_estimators\"],\n",
    "        \"best_max_leaf_nodes\": clf.best_params_[\"max_leaf_nodes\"],\n",
    "        \"cross_validation_accuracy\": clf.best_score_,\n",
    "        \"test_set_accuracy\": report[\"accuracy\"],\n",
    "        \"test_set_macro_average_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \"test_set_weighted_average_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "RFClassifier = bestRFClassifier(X, y)\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "print(\"KNN best k: \")\n",
    "print(\"KNN best p: \")\n",
    "print(\"KNN cross-validation accuracy: \")\n",
    "print(\"KNN test set accuracy: \")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"RF best n_estimators: {RFClassifier[\"best_n_estimators\"]}\")\n",
    "print(f\"RF best max_leaf_nodes: {RFClassifier[\"best_max_leaf_nodes\"]}\")\n",
    "print(f\"RF cross-validation accuracy: {RFClassifier[\"cross_validation_accuracy\"]:.4f}\")\n",
    "print(f\"RF test set accuracy: {RFClassifier[\"test_set_accuracy\"]:.4f}\")\n",
    "print(f\"RF test set macro average F1: {RFClassifier[\"test_set_macro_average_f1\"]:.4f}\")\n",
    "print(f\"RF test set weighted average F1: {RFClassifier[\"test_set_weighted_average_f1\"]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
